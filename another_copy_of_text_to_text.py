# -*- coding: utf-8 -*-
"""Another copy of Text to text.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14Y2P46KenV4qh3nVrObbJXx4iOLIcUld
"""

import nbformat

path = 'your_notebook.ipynb'  # replace if necessary

with open(path) as f:
    nb = nbformat.read(f, as_version=4)

if 'widgets' in nb['metadata']:
    del nb['metadata']['widgets']
    with open(path, 'w') as f:
        nbformat.write(nb, f)
    print("Widget metadata removed successfully.")
else:
    print("No widgets metadata found.")

!pip install -q transformers sentencepiece accelerate

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

model_name = "google/flan-t5-xl"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)

fake_claims = [
    "Drinking bleach cures COVID-19.",
    "The Earth is flat.",
    "Vaccines contain microchips.",
    "The moon landing was faked.",
    "5G towers spread coronavirus."
]


def make_prompt(claim):
    return f"Convert this fake news into a factual statement: {claim}"


def generate_factual_outputs(claims):
    for claim in claims:
        input_text = make_prompt(claim)
        input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(device)

        output_ids = model.generate(input_ids, max_length=100, temperature=0.7)
        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

        print(f"ðŸ“Œ Fake: {claim}\nâœ… Factual: {output_text}\n")

generate_factual_outputs(fake_claims)

fake_claims = ["Drinking hot water kills the coronavirus.",
    "The Earth is flat and NASA faked all the space missions.",
    "Wearing magnets can cure arthritis.",
    "Climate change is a hoax invented by scientists.",
    "5G towers spread the coronavirus.",
    "Vaccines cause autism in children.",
    "The Great Wall of China is visible from space with the naked eye.",
    "You only use 10% of your brain.",
    "Eating carrots dramatically improves your night vision.",
    "The earth is round"

]
def make_prompt(claim):
    return f"Convert this fake news into a factual statement: {claim}"


def generate_factual_outputs(claims):
    for claim in claims:
        input_text = make_prompt(claim)
        input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(device)

        output_ids = model.generate(input_ids, max_length=100, temperature=0.7)
        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

        print(f"ðŸ“Œ Fake: {claim}\nâœ… Factual: {output_text}\n")

generate_factual_outputs(fake_claims)

